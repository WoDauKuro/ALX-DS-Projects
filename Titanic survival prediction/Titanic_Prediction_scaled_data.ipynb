{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlLTelk9lQmW"
      },
      "source": [
        "<div align=\"center\"><h1><b>Global Changemakers Study Group</b></h1></div>\n",
        "\n",
        "<div align=\"center\"><h2><b>Practical Session</b></h2></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEF6NAPplQmX"
      },
      "source": [
        "# Titanic Dataset: Building Classification Models, Hyperparameter Tuning, and Model Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRFFkVeGlQmY"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this practical presentation session, we will explore the process of building and validating classification models using the Titanic dataset. The objective is to predict the survival of passengers based on various features such as age, gender, class, and other relevant attributes. This session will guide you through the steps of model building, hyperparameter tuning, and model validation using three different machine learning models: Support Vector Machine (SVM), Random Forest Classifier, and Feedforward Neural Network.\n",
        "\n",
        "## Objectives\n",
        "- **Data Cleaning**: Create a function that takes in the data and cleans the data.\n",
        "- **Data Scaling**: Scale the data using sklearn's StandardScaler\n",
        "- **Model Building**: Construct three classification models: SVM, Random Forest, and Feedforward Neural Network using `scikit-learn` and `keras` libraries.\n",
        "- **Initial Evaluation with Train-Test Split**: Perform an initial evaluation by splitting the dataset into training and testing sets.\n",
        "- **Hyperparameter Tuning with Cross-Validation**: Optimize the models using Grid Search with Cross-Validation to tune hyperparameters.\n",
        "- **Final Evaluation on Test Set**: Confirm the models' performance on the test set to ensure generalizability.\n",
        "\n",
        "## Libraries\n",
        "\n",
        "The following libraries will be used in this session:\n",
        "\n",
        "- `numpy`: For numerical computations.\n",
        "- `pandas`: For data manipulation and analysis.\n",
        "- `scikit-learn`: For building and evaluating machine learning models.\n",
        "- `keras`: For constructing and training neural networks.\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "The pre-cleaned Titanic dataset provides information on the passengers of the Titanic ship. Key features include:\n",
        "\n",
        "- `Ticket`: Ticket number. (Removed after data cleaning)\n",
        "- `Name`: Passenger's name. (Removed after data cleaning)\n",
        "- `PassengerId`: Unique identifier for each passenger. (Removed after data cleaning)\n",
        "- `Cabin`: Cabin number. (Removed after data cleaning)\n",
        "- `Survived`: Survival indicator (0 = No, 1 = Yes).\n",
        "- `Pclass`: Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd).\n",
        "- `Sex`: Gender of the passenger.\n",
        "- `Age`: Age of the passenger.\n",
        "- `Fare`: Passenger fare.\n",
        "- `SibSp`: Number of siblings/spouses aboard the Titanic.\n",
        "- `Parch`: Number of parents/children aboard the Titanic.\n",
        "- `Family_Size`: Number of individuals in each family. Created by adding `Parch` and `SibSp`.\n",
        "- `Embarked`: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).\n",
        "- `Title`: one-hot encoded `Title` for each passenger. Created from the passenger names in the original dataset.\n",
        "\n",
        "## Key Steps in the Workflow\n",
        "\n",
        "1. **Load and clean the Dataset**:\n",
        "    - We will be creating a function that loads and cleans the dataset to ensure that it is ready for model building and evaluation. Preprocessing steps such as handling missing values, and encoding categorical variables will be done within the function.\n",
        "\n",
        "2. **Scale Data**:\n",
        "    - After building and validating the models, we will scale the data and perform steps 2-5 on the scaled data to assess if there is any improvement in performance.\n",
        "\n",
        "2. **Model Building**:\n",
        "    - We will build three classification models: SVM, Random Forest, and Feedforward Neural Network using the `scikit-learn` and `keras` libraries.\n",
        "\n",
        "3. **Initial Evaluation of Models**:\n",
        "    - We will perform an initial evaluation by splitting the dataset into training and testing sets. This will give us a quick overview of the model performance on unseen data.\n",
        "\n",
        "4. **Hyperparameter Tuning with Cross-Validation**:\n",
        "    - To optimize our models, we will use Grid Search with Cross-Validation to tune the hyperparameters. This step ensures that we select the best set of hyperparameters that maximize model performance.\n",
        "\n",
        "5. **Final Evaluation on Test Set**:\n",
        "    - After tuning the hyperparameters, we will retrain the models using the best parameters on the entire training set. The final evaluation will employ cross-validation on the training set to assess the models' performance and generalizability.\n",
        "\n",
        "\n",
        "\n",
        "## Metrics for Evaluation\n",
        "\n",
        "We will compare the models' performance based on various metrics such as accuracy, precision, recall, and F1-score. These metrics will help us understand the strengths and weaknesses of each model in predicting passenger survival.\n",
        "\n",
        "This comprehensive approach to model building and validation aims to ensure that our models are not only well-tuned but also generalize effectively to unseen data, providing robust and reliable predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59kgSDqWlQmZ"
      },
      "source": [
        "# Table of Contents:\n",
        "\n",
        "- **1. [Load Dependencies](#load-dependencies)**\n",
        "\n",
        "- **2. [Load Data, Create a function to clean the Data, Separate dataset and Execute train-test split](#load-clean-separate-data)**\n",
        "  - **2.1. [Load Data](#load)**\n",
        "  - **2.2. [Create function](#clean)**\n",
        "  - **2.3. [Separate Data](#separate)**\n",
        "\n",
        "- **3. [Build Models](#Model-Building)**\n",
        "\n",
        "- **4. [Initial Evaluation of Models](#Evaluation)**\n",
        "\n",
        "- **5. [Tuning Model Hyperparameters](#Hyperparameter-tuning)**\n",
        "\n",
        "- **6. [Final Evaluation of Models](#Evaluation)**\n",
        "\n",
        "- **7. [Conclusion](#Conclusion)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW4jwSVklQmZ"
      },
      "source": [
        "<a id=\"load-dependencies\"></a>\n",
        "\n",
        "# 1. Load dependencies\n",
        "As usual, the first step in a python data project is to import the necessary Python packages we will be utilising during the practical session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPtQ-KfZlQmZ",
        "outputId": "222b9f0a-f5be-4e67-9aca-75bd4cd0dde9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting keras>=3.2.0 (from scikeras)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=1.4.2 (from scikeras)\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
            "Collecting namex (from keras>=3.2.0->scikeras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.9.0)\n",
            "Collecting optree (from keras>=3.2.0->scikeras)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Installing collected packages: namex, optree, scikit-learn, keras, scikeras\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.3.3 namex-0.0.8 optree-0.11.0 scikeras-0.13.0 scikit-learn-1.4.2\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# run the code below if you do not have `scikeras`\n",
        "!pip install scikeras\n",
        "\n",
        "# 'scikeras' is a library that extends the capabilities of scikit-learn by providing compatibility between scikit-learn and Keras, a popular deep learning framework.\n",
        "# It offers utilities to seamlessly integrate Keras models into scikit-learn workflows.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split,  cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from scikeras.wrappers import KerasClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "915rfnCqlQma"
      },
      "source": [
        "<a id=\"load-clean-separate-data\"></a>\n",
        "\n",
        "# 2. Load Data, Create a function to clean the Data, Separate dataset and Execute train-test split\n",
        "The Titanic dataset is imported and loaded into a Pandas DataFrame for manipulation and analysis. The target variable contains missing values, which need to be handled since machine learning models cannot accept them. The dataset is separated into train (with non-null target values) and test (with null target values) variables. The train data is used for training models, while the test data is used for making predictions after training. The train data is then split into training and validation sets following an 80-20 rule to ensure the model generalizes well and the evaluation metrics are reliable.\n",
        "\n",
        "<a id=\"load\"></a>\n",
        "## 2.1. Load Data\n",
        "We start by importing the Titanic dataset, which is stored in a CSV file format. Loading the data into a Pandas DataFrame allows us to easily manipulate and analyze it using Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fT61tXjmlQma"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"https://raw.githubusercontent.com/WoDauKuro/ALX-DS-Projects/main/Titanic%20survival%20prediction/train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/WoDauKuro/ALX-DS-Projects/main/Titanic%20survival%20prediction/test.csv\")\n",
        "# Concatenate `train` and `test`\n",
        "df_scaled = pd.concat([train, test], axis=0, sort=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPo8hMHSlQma",
        "outputId": "ff2ff812-3b41-40ca-a41c-09c356d11dfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1309 entries, 0 to 417\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   Age          1046 non-null   float64\n",
            " 1   Cabin        295 non-null    object \n",
            " 2   Embarked     1307 non-null   object \n",
            " 3   Fare         1308 non-null   float64\n",
            " 4   Name         1309 non-null   object \n",
            " 5   Parch        1309 non-null   int64  \n",
            " 6   PassengerId  1309 non-null   int64  \n",
            " 7   Pclass       1309 non-null   int64  \n",
            " 8   Sex          1309 non-null   object \n",
            " 9   SibSp        1309 non-null   int64  \n",
            " 10  Survived     891 non-null    float64\n",
            " 11  Ticket       1309 non-null   object \n",
            "dtypes: float64(3), int64(4), object(5)\n",
            "memory usage: 132.9+ KB\n"
          ]
        }
      ],
      "source": [
        "# Check the information of df\n",
        "df_scaled.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggXYN9xqlQmb"
      },
      "source": [
        "<a id=\"clean\"></a>\n",
        "## 2.2. Create function\n",
        "After loading the data, you will observe that there are several problems with the data; missing values, unimportant columns, categorical values etc. To handle these problems, we would create a function that takes in the dataframe, cleans it and returns the cleaned dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3tX3bx9MlQmb"
      },
      "outputs": [],
      "source": [
        "def titan_sweep(df_scaled):\n",
        "    # create new Title column\n",
        "    df_scaled['Title'] = df_scaled['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n",
        "\n",
        "    # replace rare titles with more common ones\n",
        "    mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr',\n",
        "           'Don': 'Mr', 'Mme': 'Mrs', 'Jonkheer': 'Mr', 'Lady': 'Mrs',\n",
        "           'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n",
        "    df_scaled.replace({'Title': mapping}, inplace=True)\n",
        "\n",
        "    # impute missing Age values using median of Title groups\n",
        "    title_ages = dict(df_scaled.groupby('Title')['Age'].median())\n",
        "\n",
        "    # create a column of the median ages\n",
        "    df_scaled['age_med'] = df_scaled['Title'].apply(lambda x: title_ages[x])\n",
        "\n",
        "    # replace all missing ages with the value in this column\n",
        "    df_scaled['Age'].fillna(df_scaled['age_med'], inplace=True, )\n",
        "    df_scaled.drop(columns='age_med', inplace=True)\n",
        "\n",
        "    # impute missing Fare values using median of Pclass groups\n",
        "    class_fares = dict(df_scaled.groupby('Pclass')['Fare'].median())\n",
        "\n",
        "    # create a column of the average fares\n",
        "    df_scaled['fare_med'] = df_scaled['Pclass'].apply(lambda x: class_fares[x])\n",
        "\n",
        "    # replace all missing fares with the value in this column\n",
        "    df_scaled['Fare'].fillna(df_scaled['fare_med'], inplace=True, )\n",
        "    df_scaled.drop(columns='fare_med', inplace=True)\n",
        "\n",
        "    # Impute missing \"embarked\" value\n",
        "    df_scaled['Embarked'].fillna(method='backfill', inplace=True)\n",
        "\n",
        "    #  create Family_Size column\n",
        "    df_scaled['Family_Size'] = df_scaled['Parch'] + df_scaled['SibSp']\n",
        "\n",
        "    # convert to category dtype and then encode with numbers using `.cat.codes` attribute\n",
        "    df_scaled['Sex'] = df_scaled['Sex'].astype('category').cat.codes\n",
        "\n",
        "    # Encode other categorical variables using one-hot encoding\n",
        "    categorical = ['Embarked', 'Title']\n",
        "    for item in categorical:\n",
        "        df_scaled = pd.concat([df_scaled,\n",
        "                        pd.get_dummies(df_scaled[item], prefix=item)], axis=1)\n",
        "        df_scaled.drop(columns=item, inplace=True)\n",
        "\n",
        "    # drop the variables we won't be using\n",
        "    df_scaled.drop(['Cabin', 'Name', 'Ticket', 'PassengerId'], axis=1, inplace=True)\n",
        "\n",
        "    return df_scaled\n",
        "\n",
        "# Implement the function\n",
        "df_scaled = titan_sweep(df_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgoB0iUQlQmb",
        "outputId": "0ebceaa9-0051-4d8f-c8d1-05619f274faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1309 entries, 0 to 417\n",
            "Data columns (total 17 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Age           1309 non-null   float64\n",
            " 1   Fare          1309 non-null   float64\n",
            " 2   Parch         1309 non-null   int64  \n",
            " 3   Pclass        1309 non-null   int64  \n",
            " 4   Sex           1309 non-null   int8   \n",
            " 5   SibSp         1309 non-null   int64  \n",
            " 6   Survived      891 non-null    float64\n",
            " 7   Family_Size   1309 non-null   int64  \n",
            " 8   Embarked_C    1309 non-null   bool   \n",
            " 9   Embarked_Q    1309 non-null   bool   \n",
            " 10  Embarked_S    1309 non-null   bool   \n",
            " 11  Title_Dr      1309 non-null   bool   \n",
            " 12  Title_Master  1309 non-null   bool   \n",
            " 13  Title_Miss    1309 non-null   bool   \n",
            " 14  Title_Mr      1309 non-null   bool   \n",
            " 15  Title_Mrs     1309 non-null   bool   \n",
            " 16  Title_Rev     1309 non-null   bool   \n",
            "dtypes: bool(9), float64(3), int64(4), int8(1)\n",
            "memory usage: 94.6 KB\n"
          ]
        }
      ],
      "source": [
        "# Check the information of the cleaned df\n",
        "df_scaled.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq55Umc0lQmb"
      },
      "source": [
        "<a id=\"separate-scale\"></a>\n",
        "## 2.3. Separate Data\n",
        "After cleaning our data, we still observe missing values for the `Survived` column. We left that column as is because it is our target variable and tampering with it would not be good for model performance i.e. our model would be predicting for values that do not represent the true values.\n",
        "Since machine learning models do not accept missing values, we need to remove or handle rows with missing values in the target variable before proceeding.\n",
        "\n",
        "A good approach to handling this issue is to separate the dataframe into `train` and `test`variables. The `train` variable will contain the data with non-null values in the target variable and will be used for training the machine learning models. The `test` variable will contain the features for rows where the target variable is null and will be used for testing or prediction after the model is trained. This approach ensures that we have a complete target variable for training while still being able to make predictions for the rows with missing target values.\n",
        "\n",
        "With the dataset separated into `train` and `test` variables, the next step is to split the data into training and validation sets. We adhere to the 80-20 rule, where approximately 80% of the data is used for training and 20% for testing. This split helps us assess how well the model generalizes to new, unseen data and ensures that our evaluation metrics are reliable.\n",
        "\n",
        "\n",
        "# Data Scaling in Model Building\n",
        "\n",
        "Data scaling, also known as feature scaling or normalization, is a crucial preprocessing step in machine learning. It involves transforming the numerical features of a dataset to a similar scale.\n",
        "\n",
        "#### Importance of Data Scaling in Model Building:\n",
        "\n",
        "1. Ensures that all features contribute equally to the result.\n",
        "2. Helps algorithms converge faster.\n",
        "3. Prevents features with larger ranges from dominating the learning process.\n",
        "\n",
        "\n",
        "### Understanding the importance of data scaling, let us now Scale our data to see how our models perform.\n",
        "\n",
        "\n",
        "**NOTE:**\n",
        "Data leakage is a serious issue as it undermines the validity of the model evaluation.\n",
        "\n",
        "Data leakage occurs when information from outside the training dataset is used to create the model, which can lead to overly optimistic performance estimates and a model that fails to generalize to new, unseen data.\n",
        "\n",
        "Data Leakage can occur when information from the test set leaks into the training set, often due to improper data splitting or preprocessing.\n",
        "\n",
        "Example: Scaling the entire dataset before splitting into train and test sets can cause the test set information to influence the training process.\n",
        "\n",
        "**Always split your dataset into training, validation, and test sets before any preprocessing.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2UGN3xFBlQmb"
      },
      "outputs": [],
      "source": [
        "# Separate data into feature (`X`) and target variables (`y`)\n",
        "train = df_scaled[pd.notnull(df_scaled['Survived'])]\n",
        "X_test = df_scaled[pd.isnull(df_scaled['Survived'])].drop(['Survived'], axis=1)\n",
        "\n",
        "col_to_scale = ['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Family_Size']\n",
        "\n",
        "# Execute train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(train.drop(['Survived'], axis=1), train['Survived'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train[col_to_scale] = scaler.fit_transform(X_train[col_to_scale])\n",
        "\n",
        "# Transform the validation data\n",
        "X_val[col_to_scale] = scaler.transform(X_val[col_to_scale])\n",
        "\n",
        "# Transform the test data\n",
        "X_test[col_to_scale] = scaler.transform(X_test[col_to_scale])\n",
        "\n",
        "# Now X_train, X_val, y_train, y_val, and X_test2 are ready for modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Tfhn58lQmb"
      },
      "source": [
        "<a id=\"Model-Building\"></a>\n",
        "\n",
        "# 3. Build Models\n",
        "In this crucial step, we embark on constructing our predictive models! Recognizing the distinctive construction method of the feedforward neural network compared to the SVM and random forest models, we initiate by crafting a dedicated function specifically tailored for the neural network. Subsequently, we proceed to define this neural network alongside the other models, facilitating a cohesive framework for comparative analysis and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PbdrZc81lQmc"
      },
      "outputs": [],
      "source": [
        "# Define custom scorer for Keras model\n",
        "def create_ffnn():\n",
        "    \"\"\"\n",
        "    Constructs a Feedforward Neural Network (FFNN) model using Keras Sequential API.\n",
        "\n",
        "    Returns:\n",
        "    model : Sequential\n",
        "        A compiled FFNN model ready for training and evaluation.\n",
        "\n",
        "    Notes:\n",
        "    - This function defines a simple FFNN architecture with three fully connected layers.\n",
        "    - The input layer size is automatically determined based on the input data dimension.\n",
        "    - The hidden layers use Rectified Linear Unit (ReLU) activation function for introducing non-linearity.\n",
        "    - The output layer uses a sigmoid activation function, suitable for binary classification tasks.\n",
        "    - The model is compiled with binary cross-entropy loss function, Adam optimizer, and accuracy metric.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define models\n",
        "svm_model = SVC()\n",
        "rf_model = RandomForestClassifier()\n",
        "ffnn_model = KerasClassifier(build_fn=create_ffnn, epochs=100, batch_size=10, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMzr1g43lQmc"
      },
      "source": [
        "<a id=\"Evaluation\"></a>\n",
        "\n",
        "# 4. Initial Evaluation of Models\n",
        "Initial Evaluation with Train-Test Split\" serves as our first step in assessing the performance of various machine learning models on our dataset. In this evaluation, we employ a train-test split methodology to train multiple models and evaluate their predictive capabilities. The models under scrutiny include Support Vector Machine, Random Forest, and a Feedforward Neural Network. For each model, we measure key performance metrics such as accuracy, precision, recall, and F1-score. This initial evaluation provides valuable insights into the effectiveness of different algorithms in predicting outcomes and guides further optimization and refinement of our predictive models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT-1oa6OlQmc",
        "outputId": "ebf35230-c432-4f82-8a24-d8016ae7f8d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Evaluation with Train-Test Split\n",
            "Model: Support Vector Machine\n",
            "Accuracy:  0.8268156424581006\n",
            "Precision:  0.8115942028985508\n",
            "Recall:  0.7567567567567568\n",
            "F1-score:  0.7832167832167832\n",
            "\n",
            "\n",
            "Model: Random Forest\n",
            "Accuracy:  0.8379888268156425\n",
            "Precision:  0.8082191780821918\n",
            "Recall:  0.7972972972972973\n",
            "F1-score:  0.8027210884353742\n",
            "\n",
            "\n",
            "Model: Feedforward Neural Network\n",
            "Accuracy:  0.8268156424581006\n",
            "Precision:  0.8412698412698413\n",
            "Recall:  0.7162162162162162\n",
            "F1-score:  0.7737226277372263\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initial Evaluation with Train-Test Split\n",
        "print(\"Initial Evaluation with Train-Test Split\")\n",
        "for model_name, model in [(\"Support Vector Machine\", svm_model), (\"Random Forest\", rf_model), (\"Feedforward Neural Network\", ffnn_model)]:\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred_val = model.predict(X_val)\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(\"Accuracy: \", accuracy_score(y_val, y_pred_val))\n",
        "    print(\"Precision: \", precision_score(y_val, y_pred_val))\n",
        "    print(\"Recall: \", recall_score(y_val, y_pred_val))\n",
        "    print(\"F1-score: \", f1_score(y_val, y_pred_val))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcmWv175lQmc"
      },
      "source": [
        "<a id=\"Hyperparameter-tuning\"></a>\n",
        "\n",
        "# 5. Tuning Model Hyperparameters\n",
        "Cross-Validation with Hyperparameter Tuning on Training Set\" introduces a pivotal stage in refining our machine learning models for optimal performance. Here, we employ a rigorous approach known as cross-validation coupled with hyperparameter tuning to systematically explore various model configurations. We define parameter grids tailored to each algorithm—Support Vector Machine, Random Forest, and Feedforward Neural Network—to explore different combinations of hyperparameters. Utilizing GridSearchCV, a method for hyperparameter optimization, we exhaustively search through these parameter grids while evaluating performance using a cross-validation strategy on the training set. By doing so, we identify the most effective model configurations that maximize predictive accuracy. This meticulous process ensures that our models are finely tuned to capture the complexities of the dataset, enhancing their generalization and predictive power."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJfSpkxMlQmc",
        "outputId": "9bde7710-1b4b-43f1-d820-ed5be0449e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation with Hyperparameter Tuning on Training Set\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 24 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x796ab8b58310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x796ab8434b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters:\n",
            "Support Vector Machine:  {'C': 1, 'gamma': 5, 'kernel': 'linear'}\n",
            "Random Forest:  {'max_depth': 5, 'n_estimators': 100}\n",
            "Feedforward Neural Network:  {'batch_size': 10, 'epochs': 50}\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter Tuning using Cross-Validation on Training Set\n",
        "print(\"Cross-Validation with Hyperparameter Tuning on Training Set\")\n",
        "\n",
        "# Define parameter grids\n",
        "svm_param_grid = {'C': [0.1, 1, 10, 15], 'kernel': ['linear', 'rbf'], 'gamma': [5, 10, 30, 50]}\n",
        "rf_param_grid = {'n_estimators': [50, 100, 200, 350], 'max_depth': [5, 10, 20, 35]}\n",
        "ffnn_param_grid = {'epochs': [50, 100, 200, 350], 'batch_size': [10, 20, 30, 40]}\n",
        "\n",
        "# Define GridSearchCV\n",
        "svm_grid_search = GridSearchCV(svm_model, svm_param_grid, scoring='accuracy', cv=5)\n",
        "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, scoring='accuracy', cv=5)\n",
        "ffnn_grid_search = GridSearchCV(ffnn_model, ffnn_param_grid, scoring='accuracy', cv=5)\n",
        "\n",
        "# Fit models\n",
        "svm_grid_search.fit(X_train, y_train)\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "ffnn_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best models\n",
        "best_svm_model = svm_grid_search.best_estimator_\n",
        "best_rf_model = rf_grid_search.best_estimator_\n",
        "best_ffnn_model = ffnn_grid_search.best_estimator_\n",
        "\n",
        "print(\"Best Parameters:\")\n",
        "print(\"Support Vector Machine: \", svm_grid_search.best_params_)\n",
        "print(\"\\n\")\n",
        "print(\"Random Forest: \", rf_grid_search.best_params_)\n",
        "print(\"\\n\")\n",
        "print(\"Feedforward Neural Network: \", ffnn_grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgGOnTlklQmc"
      },
      "source": [
        "<a id=\"Evaluation\"></a>\n",
        "\n",
        "# 6. Final Evaluation of Models\n",
        "### How we usually Evaluate models\n",
        "Evaluate the model directly by comparing its predictions on unseen data with the true labels.\n",
        "### What we would do\n",
        "The final model evaluation after hyperparameter tuning marks the culmination of our model development journey. Through meticulous optimization and validation on the training set, including hyperparameter tuning, we fine-tuned our Support Vector Machine, Random Forest, and Feedforward Neural Network models. In this phase, we assess the performance of these refined models by employing cross-validation on the training set. By averaging the performance scores over multiple folds, we obtain a realistic measure of their effectiveness in real-world scenarios. This evaluation provides valuable insights into how well our models generalize to unseen data and their ability to accurately predict outcomes. It serves as a crucial validation step, ensuring that our models are robust, reliable, and ready for deployment in practical applications.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaCDnd2ilQmc",
        "outputId": "1674889a-9a43-41bd-a8b2-4e03cf571a11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Evaluation on Test Set\n",
            "Accuracy score is: Support Vector Machine: 0.8300\n",
            "Precision score is: Support Vector Machine: 0.8229\n",
            "Recall score is: Support Vector Machine: 0.8111\n",
            "F1 score is: Support Vector Machine: 0.8157\n",
            "Accuracy score is: Random Forest: 0.8371\n",
            "Precision score is: Random Forest: 0.8355\n",
            "Recall score is: Random Forest: 0.8098\n",
            "F1 score is: Random Forest: 0.8125\n",
            "Accuracy score is: Feedforward Neural Network: 0.8328\n",
            "Precision score is: Feedforward Neural Network: 0.8238\n",
            "Recall score is: Feedforward Neural Network: 0.7974\n",
            "F1 score is: Feedforward Neural Network: 0.8092\n"
          ]
        }
      ],
      "source": [
        "# Final Evaluation on Test Set after Hyperparameter Tuning\n",
        "print(\"Final Evaluation using cross validation\")\n",
        "for model_name, model in [(\"Support Vector Machine\", best_svm_model), (\"Random Forest\", best_rf_model), (\"Feedforward Neural Network\", best_ffnn_model)]:\n",
        "    acc_score = cross_val_score(model, X_train, y_train, cv=5).mean()\n",
        "    prec_score = cross_val_score(model, X_train, y_train, cv=5, scoring='precision_macro').mean()\n",
        "    reca_score = cross_val_score(model, X_train, y_train, cv=5, scoring='recall_macro').mean()\n",
        "    f1_score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_macro').mean()\n",
        "    print(f\"Accuracy score is: {model_name}: {acc_score:.4f}\")\n",
        "    print(f\"Precision score is: {model_name}: {prec_score:.4f}\")\n",
        "    print(f\"Recall score is: {model_name}: {reca_score:.4f}\")\n",
        "    print(f\"F1 score is: {model_name}: {f1_score:.4f}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"Conclusion\"></a>\n",
        "\n",
        "# 6. Conclusion\n",
        "Scaling the data using StandardScaler had a noticeable impact on the performance of the models. After scaling, the accuracy, precision, recall, and F1-score of all models improved compared to their initial evaluations. The SVM and Random Forest models, in particular, showed significant improvements, highlighting the importance of data scaling in enhancing model performance.\n",
        "Overall, this project demonstrates that standardizing the features of a dataset can lead to better model performance and more reliable predictions. The insights gained emphasize the value of data preprocessing techniques, such as scaling, in the machine learning workflow."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
